# Object-Oriented Programming

**Object-Oriented Programming (OOP)** is a way of thinking about how to organize programs. This way of thinking focuses on objects. We already know about objects from the last chapter, so what's new here?

The difference is that we're creating our own types now. In the last chapter we learned about built in types: floating point numbers, lists, arrays, functions, etc. Now we will discuss broadly how one can create his own types in both R and Python. We will not go into this too deeply, but it is important to know how how code works so that we can use it more effectively. 

Here are a few abstract concepts that will help thinking about OOP. They are not mutually exclusive, and they aren't unique to OOP, but understanding these words will help you understand the purpose of OOP. Later on, when we start looking at code examples, I will alert you to when these concepts are coming into play. 

- **Composition** refers to the idea when one type of object *contains* an object of another type. For example, a linear model object could hold onto estimated regression coefficients, residuals, etc.

- **Inheritance** takes place when an object can be considered to be of multiple types. For example, an analysis of variance linear regression model might be a special case of a general linear model. 

 - **Polymorphism** is the idea that the programmer can use the same code and get different behavior depending on what the code is being used on. For example, built-in functions in both R and Python can work on arguments of a wide variety of different types. 

 - **Encapsulation** is another word for complexity hiding. Do you have to understand every line of code in a package you're using? No, because a lot of details are hidden from you. 
 
 - **Modularity** is an idea related to encapsulation--it means splitting something into independent pieces. How you split code into different files, different functions, different classes--all of that has to do with modularity. It promotes encapsulation, and it allows you to think about only a few lines of code at a time.
 
 - The **interface**, between you and the code you're using, describes *what* can happen, but not *how* it happens. In other words, it describes some functionality so that you can decide whether you want to use it, but there are not enough details for you to make it work yourself. For example, all you have to do to be able to estimate a complicated statistical model is to look up some documentation.^[Just because you can do this, doesn't mean you *should*, though!] In other words, you only need to be familiar with the interface, not the implementation. 

- The **implementation** of some code you're using describes *how* it works in detail. If you are a package author, you can change your code's implementation "behind the scenes" and ideally, your end-users would never notice.

Note that this is just a quick overview that is suitable for us as aspiring data scientists and statisticians. I should should mention that there are plenty of other resources available to you, if you'd like to learn more TODO

## OOP In Python

### Overview

In Python, [classes](https://docs.python.org/3/tutorial/classes.html) are user-defined types. When you define your own class, you describe what kind of information it holds onto, and how it behaves.  

To define your own type, use the [`class` keyword](https://docs.python.org/3/tutorial/classes.html#class-definition-syntax). Objects created with this class are called **instances**. The behave according to the rules written in the class definition--they always have data and/or functions bundled together in the same way, but these instances do not all have the same data. 

To be more clear, classes may have the following two things in their definition.

- **Attributes** are pieces of data "owned" by an instance created by the class. 

 - **(Instance) methods** are functions "owned" by an instance created by the class. They can use and/or modify data belonging to the class. 


### A First Example

Here's a simple example. Say we are interested in calculating, from numerical data $x_1, \ldots, x_n$, a sample mean:
$$
\bar{x}_n = \frac{\sum_{i=1}^n x_i}{n}.
$$

In Python, we can calculate this one number very easily using `np.average`. However, this function requires that we pass into it all of the data. What if we don't have all the data at any given time? In other words, suppose that the data arrive intermittently. 

We might consider taking advantage of a recursive formula for the sample means.

$$
\bar{x}_n =  \frac{(n-1) \bar{x}_{n-1} + x_n}{n}
$$

How would we program this in Python? A first option: we might create a variable `my_running_ave`, and after every data point arrives, we could 

```{python, collapse = TRUE}
my_running_ave = 1.0
my_running_ave
my_running_ave = ((2-1)*my_running_ave + 3.0)/2
my_running_ave
my_running_ave = ((3-1)*my_running_ave + 2.0)/3
my_running_ave
```

There are a few problems with this. Every time we add a data point, the formula slightly changes. Every time we update the average, we have to write a different line of code. This opens up the possibility for bugs, and it makes your code less likely to be used by other people and more difficult to understand. And if we were trying to code up something more complicated than a running average? That would make matters even worse. 

A second option: write a class that holds onto the running average, and that has 

 1. an `update` method that updates the running average every time a new data point is received, and 
 2. a `get_current_xbar` method that gets the most up-to-date information for us.
 
 Our code would look like this:

```{python, collapse = TRUE, echo =FALSE}
class RunningMean:
  """Updates a running average"""
  def __init__(self):
    self.current_xbar = 0.0
    self.n = 0
  def update(self, new_x):
    self.n += 1
    self.current_xbar = (self.current_xbar*(self.n - 1) + new_x) / self.n
  def get_current_xbar(self):
    if self.n == 0:
      return None
    else:
      return self.current_xbar
```

```{python, collapse = TRUE}
my_ave = RunningMean() # create running average object
my_ave.get_current_xbar() # no data yet!
my_ave.update(1.) # first data point
my_ave.get_current_xbar() # xbar_1
my_ave.update(3.)  # second data point
my_ave.get_current_xbar()  #xbar_2
my_ave.n   # my_ave.n instead of self.n
```

That's much better! Notice the *encapsulation*. Looking at this code we don't need to think about the mathematical formula and the data being received. We only need to think about the latter. In other words, the *implementation* is separated from the *interface*. The interface in this case, is just the name of the class methods, and the arguments they expect. That's all we need to know about to use this code. 

Classes (obviously) need to be defined before they are used, so here is the definition of our class. 

```{python, collapse = TRUE}
class RunningMean:
  """Updates a running average"""
  def __init__(self):
    self.current_xbar = 0.0
    self.n = 0
  def update(self, new_x):
    self.n += 1
    self.current_xbar = (self.current_xbar*(self.n - 1) + new_x) / self.n
  def get_current_xbar(self):
    if self.n == 0:
      return None
    else:
      return self.current_xbar
```

```{block, type='rmd-details'}
Methods that look like `\_\_init\_\_`, or that possess names that begin and end with two underscores, are called **dunder (double underscore) methods**, **special methods** or **magic methods**. There are many that you can take advantage of! For more information see [this](https://docs.python.org/3/reference/datamodel.html#special-method-names).
```

Here are the details of the class definition:

1. Defining class methods looks exactly like defining functions! The primary difference is that the first argument must be `self`. If the definition of a method refers to `self`, then this allows the class instance to refer to its own (heretofore undefined) data attributes. Also, these method definitions are indented inside the definition of the class. 

2. This class creates two data attributes. One to represent the number of data points seen up to now (`n`), and another to represent the current running average (`current_xbar`). 

3. Referring to data members requires dot notation. `self.n` refers to the `n` belonging to any instance. This data attribute is free to vary between all the objects instantiated by this class. 
4. The `__init__` method performs the setup operations that are performed every time any object is instantiated. 

5. The `update` method provides the core functionality using the recursive formula displayed above.

6. `get_current_xbar` simply returns the current average. In the case that this function is called before any data has been seen, it returns `None`.

A few things you might find interesting:

1. Computationally, there is never any requirement that we must hold *all* of the data points in memory. Our data set could be infinitely large, and our class will hold onto only one floating point number, and one integer. 

2. This example is generalizable to other statistical methods. In a mathematical statistics course, you will learn about a large class of models having *sufficient statistics*. Most sufficient statistics have recursive formulas like the one above. Second, many algorithms in *time series analysis* have recursive formulas and are often needed to analyze large streams of data. They can all be wrapped into a class in a way that is similar to the above example.

### Adding Inheritance


Last, we'll talk about inheritance. In my opinion, it isn't a critical data analysis tool. However, it is helpful to understand a little bit about Python's inheritance so that we can compare it to R's inheritance system.

Let's suppose I assume that the data points $x_1, \ldots, x_n$ are a "random sample"^[Otherwise known as an independent and identically distributed sample] from a normal distibution with mean $\mu$ and variance $\sigma^2=1$. $\mu$ is assumed to be unknown (this is, after all, and interval for $\mu$), and $\sigma^2$ is assumed to be known, for simplicity. 

A $95\%$ confidence interval for the true unknown population mean $\mu$ is 
$$
\left[\bar{x} - 1.96 \sqrt{\frac{\sigma^2}{n}}, \bar{x} + 1.96 \sqrt{\frac{\sigma^2}{n}} \right]
$$
The width of the interval shrinks as we get more data (as $n \to \infty$). We can write another class that, not only calculates the center of this interval, $\bar{x}$, but also returns the interval endpoints. 

If we wrote another class from scratch, then we would need to rewrite a lot of the code that we already have in the definition of `RunningMean`. Instead, we'll use the idea of [*inheritance*](https://docs.python.org/3/tutorial/classes.html#inheritance). 

```{python, collapse = TRUE}
import numpy as np
class RunningCI(RunningMean):
  """Updates a running average and gives you a known-variance confidence interval"""
  def __init__(self, known_var):
    super().__init__()
    self.known_var = known_var
  def get_current_interval(self):
    if self.n == 0:
      return None
    else:
      half_width = 1.96 * np.sqrt(self.known_var / self.n)  
      return np.array([self.current_xbar - half_width, self.current_xbar + half_width])
```

The parentheses in the first line signal that this new class definition is inheriting from `RunningMean`. Inside the definition of this new class, when I refer to `self.current_xbar` it knows what I'm talking about because these are defined in the base class. Last, I am using `super()` to access the parent class, which lets me use the parent class' methods, such as `__init__`.

```{python, collapse = TRUE}
my_ave = RunningCI(1) # create running average object
my_ave.get_current_xbar() # no data yet!
my_ave.update(1.) 
my_ave.get_current_interval() 
my_ave.update(3.)  
my_ave.get_current_interval()  
```



## In R

R, unlike Python, has many different types of classes. I will only discuss `S3` classes, and `S4` classes. 

In R's class system, there aren't functions (methods) that *belong* to a class. Instead, some functions behave differently depending on what type of object is passed into them. These decision-making functions are called **generic functions**--when you call them, they decide on another function to call. 

The person using these generic functions is using an *interface*. He/she does not care about all the details going on behind the scenes, and there is a smaller barrier to entry to getting things done. `print`, `summary`, and `plot` are generic functions. 

```{r}
plot
```

The body of this function is one-line. It calls something called `UseMethod`. A **method** is a function that gets chosen by the generic function for a particular type of input. You can see the methods for the generic function `plot` by typing the following. 

```{r}
methods(plot)
```


```{block, type='rmd-caution'}
This dot-notation is nothing like Python's dot-notation! Functions do not *belong* to types/classes in R like they do in Python!
```

Which `plot` method gets chosen depends on what the argument is. Specificaly it depends on what `class` the object has. More on that below TODO

Let's look at what the `plot.data.frame` function looks like TODO


### S3 objects


https://cran.r-project.org/doc/manuals/R-lang.html#Object_002doriented-programming

https://adv-r.hadley.nz/oo.html





## Exercises

All answers to questions related to R should be written in a file named `data_types_exercises.R`. All answers to questions related to Python should be written in a file named `data_types_exercises.py`.

1. In Python, write a class that implements a running sample variance.`

2. In Python, write a class that estimates a linear regression model.

Let

- Let $\begin{bmatrix}x_1 \\ x_2 \\ \vdots \\ x_{10} \end{bmatrix}$ be a $10 \times 1$ vector of predictor variables.
- $\mathbf{X} = \begin{bmatrix} 1 & x_1 \\ 1 & x_2 \\ \vdots & \vdots \\ 1 & x_{10} \end{bmatrix}$ be a $10 \times 2$ "design matrix", 
- $\mathbf{y} = 
\begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_{10} \end{bmatrix}$ be a $10 \times 1$ vector of dependent observations, and
- $\boldsymbol{\epsilon} = \begin{bmatrix}
\epsilon_1 \\
\epsilon_2 \\
\vdots \\
\epsilon_{10} \end{bmatrix}$ be a $10 \times 1$ vector of mean $0$ random errors. The assumed regression model can be written as

$$
\mathbf{y} = 
\mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon},
$$
and the estimate for the coefficient vector can be written as
$$
\hat{\boldsymbol{\beta}} = \left(\mathbf{X}^\intercal \mathbf{X} \right)^{-1}\mathbf{X}^\intercal \mathbf{y}.
$$

- Make the class called `LinearModel`, and use only the numpy package 
- have the only attribute be a numpy array of estimated coefficients `coeffs`
- have one method called `fit` that takes a numpy arrays `X` (the design matrix), and `y`
- every time `fit` is called, reset the `coeffs` using the formula above



```{python, collapse = TRUE}
import numpy as np
import matplotlib.pyplot as plt

# don't hardcode variables!
num_rows = 10
num_predictors = 1
num_x_columns = num_predictors + 1

# generate fake data 
true_coefficients = np.array([1,-3]).reshape((num_x_columns,1))
x_array = np.empty((num_rows, num_x_columns))
x_array[:,0] = 1
x_array[:,1] = np.random.normal(size=num_rows)
y_array = np.dot(x_array, true_coefficients) 
y_array = y_array + np.random.normal(scale = .3, size = num_rows).reshape((num_rows,1))

# plot fake data 
plt.scatter(x_array[:,1], y_array)
```


Here is an example class definition, and then its use. The class  holds onto

