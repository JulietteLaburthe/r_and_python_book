# Odds and Ends

## Input and Output

So far we have been creating small pieces of data within our scripts. This is primarily for pedagogical purposes. In real life, we can have 

- data read in from a data set saved on our machine's hard drive (e.g. `my_data.csv`),
- data read in from a data base (e.g. MySQL, PostgreSQL, etc.), or
- data created in a script (either deterministic or random).

I focus mostly on the first one. The third is handled with basic assignment, and using specialized functions that are easily found. I avoid the second one because it requires my setting up a data base and teaching you SQL commands.

After we have created something useful, we might be interested in storing our results. We can write out to a database, a text file, or we can save a digitized version of our work space.

### Reading In Text Files


## Using Third-Party Code

Before using third-party code, it must first be installed. After it is installed, it must be "loaded in" to your session. I will describe both of these steps in R and Python.


### Installing Packages In R 

In R, there are thousands of user-created **packages.** You can download most of these from the [*Comprehensive R Archive Network*](https://cran.r-project.org/). You can also download packages from other publishing platforms like [Bioconductor](https://www.bioconductor.org/), or [Github](https://github.com/). Installing from CRAN is more commonplace, and extremely easy to do. Just use the `install.packages()` function. This can be run inside your R console, so there is no need to type things into the command line.

```{r, eval = FALSE}
install.packages("thePackage")
```

### Installing Packages In Python 

In Python, installing packages is more complicated. Commands must be written in the command line, and there are multiple package managers. This isn't surprising, because Python is used more extensively than R in fields other than data science.

If you followed the suggestions provided in \@ref(installing-python-by-installing-anaconda), then you installed Anaconda. This means you will usually be using the [`conda` command](https://docs.anaconda.com/anaconda/user-guide/tasks/install-packages/). Point-and-click interfaces are made available as well. 

```{bash, eval=FALSE}
conda install the_package
```

There are some packages that will not be available using this method. For more information on that situation, see [here.](https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-pkgs.html#install-non-conda-packages)

### Loading Packages In R 

After they are installed on your machine, third-party code will need to be "loaded" into your R or Python session. 

Loading in a package is relatively simple in R, however complications can arise when different variables share the same name. This happens relatively often because

- it's easy to create a variable in the global environment that has the same name as another object you don't know about, and
- different packages you load in sometimes share names accidentally.

Starting off with the basics, here's how to load in a package of third-party code. Just type the following into your R console.

```{r, eval = FALSE}
library(thePackage)
```

You can also use the `require()` function, which has slightly different behavior when the requested package is not found. 

To understand this more deeply, we need to talk about **environments** again. We discussed these before in  \@ref(more-details-on-rs-user-defined-functions), but only in the context of user-defined functions. When we load in a package with `library()`, we make its contents available by putting it all in an environment for that package. 

An [environment](https://cran.r-project.org/doc/manuals/R-lang.html#Environment-objects) holds the names of objects. There are usually several environments, and each holds a different set of functions and variables. All the variables you define are in an environment, every package you load in gets its own environment, and all the functions that come in R pre-loaded have their own environment. 

```{block, type='rmd-details'}
Formally, each environment is pair of two things: a **frame** and an **enclosure**. The frame is the set of symbol-value pairs, and the enclosure is a pointer to the parent environment. If you've heard of a *linked list* in a computer science class, it's the same thing. 
```

Moreover, all of these environments are connected in a chain-like structure.  To see what environments are loaded on your machine, and what order they were loaded in, use the `search()` function. This displays the [**search path**](https://cran.r-project.org/doc/manuals/R-lang.html#Search-path), or the ordered sequence of all of your environments.

```{r, collapse=TRUE}
search()
```

Alternatively, if you're using RStudio, the search path, and the contents of each of its environments, are displayed in the "Environment" window. You can choose which environment you'd like to look at by selecting it from the dropdown menu. This allows you to see all of the variables in that particular environment. The **global environment** (i.e. `".GlobalEnv"`) is displayed by default, because that is where you store all the objects you are creating in the console.

```{r rstudio_disp, fig.cap='The Environment Window in RStudio', out.width='80%', fig.asp=.75, fig.align='center', echo=F}
knitr::include_graphics("pics/environments_display_rstudio.png")
```

When you call `library(thePackage)`, the package has an environment created for it, and it is *inserted between the global environment, and the most recently loaded package.* When you want to access an object by name, R will first search the global environment, and then it will traverse the environments in the search path in order. These has a few important implications.

 - First, **don't define variables in the global environment that are already named in another environment.** There are many variables that come pre-loaded in the `base` package (to see them, type `ls("package:base")`), and if you like using a lot of packages, you're increasing the number of names you should avoid using. 

 - Second, **don't `library` in a package unless you need it, and if you do, be aware of all the names it will mask it packages you loaded in before**. The good news is that `library` will often print warnings letting you know which names have been masked. The bad news is that it's somewhat out of your control--if you need two packages, then they might have a shared name, and the only thing you can do about it is watch the ordering you load them in.
 
 - Third, don't use `library()` inside code that is `source`'d in other files. For example, if you attach a package to the search path from within a function you defined, anybody that uses your function loses control over the order of packages that get attached. 

All is not lost if there is a name conflict. The variables haven't disappeared. It's just slightly more difficult to refer to them. For instance, if I load in `Hmisc`, I get the warning warning that `format.pval` and `units` are now masked because they were names that were in `"package:base"`. I can still refer to these masked variables with the double colon operator (`::`).

```{r, collapse = TRUE}
library(Hmisc)
# format.pval refers to Hmisc's format.pval because it was loaded more recently
# Hmisc::format.pval in this case is the same as above
# base::format.pval this is the only way you can get base's format.pval function
```



### Loading Packages In Python 

In Python, you use the `import` statement to access objects defined in another file. It is slightly more complicated than R's `library()` function, but it is also more flexible. To make the contents of a package called, say, `the_package` available, type *one of the following* inside a Python session. 

```{python, eval = FALSE}
import the_package
import the_package as tp 
from the_package import *
```

To describe the difference between these three approaches, as well as to highlight the important takeaways and compare them with the important takeaways in the last section, we need to discuss what a Python module is, what a package is, and what a Python namespace is.^[I am avoiding any mention of *R's* namespaces and modules. These are things that exist, but they are different from Python's namespaces and modules, and are not within the scope of this text.] 

 - A Python [**`module`**](https://docs.python.org/3/tutorial/modules.html) is a separate (when I say separate, I mean separate from the script file you're currently editing) `.py` file with function and/or object definitions in it.^[The scripts you write are modules. They come with the intention of being run from start to finish. Other non-script modules are just a bag of definitions to be used in other places.]
 
 - A [package](https://docs.python.org/3/tutorial/modules.html#packages) is a group of modules.^[Sometimes a package is called a *library* but I will avoid this terminology.] 
 
 - A [**namespace**](https://docs.python.org/3/tutorial/classes.html#python-scopes-and-namespaces) is "a mapping from names to objects."
 
With these definitions, we can define `import`ing. According to the [Python documentation](https://docs.python.org/3/reference/import.html#the-import-system), "[t]he import statement combines two operations; it searches for the named module, then it binds the results of that search to a name in the local scope." 

The sequence of places Python looks for a module is called the search path. This is not the same as R's search path, though. In Python, the search path is a list of places to look for *modules*, not a list of places to look for variables. To see it, `import sys`, then type `sys.path`.

After a module is found, the variable names in the found module become available in the `import`ing module. These variables are available in the global scope, but the names you use to access them will depend on what kind of `import` statement you used. From there, you are using the same scoping rules that we described in \@ref(function-scope-in-python), which means the LEGB acronym still applies. 

Here are a few important takeaways that might not be readily apparent:

 - Python namespaces are unlike R environments in that they are not arranged into a sorted list.
 
 - Unlike in R, there is no *masking*, and you don't have to worry about the *order* of `import`ing things. 

 - However, you do have to worry about *how* you're `import`ing things. If you use the `from the_package import thingone, thingtwo` format of `import`ing, you are at risk of re-assigning either `thingone` or `thingtwo`, if they already exist. As a rule of thumb, **you should never use this form of `import`ing**.
 
- These differences might explain why Python packages tend to be larger than R packages. 

#### `import`ing Examples

In the example below, we import the entire `numpy` package in a way that lets us refer to it as `np`. This reduces the amount of typing that is required of us, but it also protects against variable name clashing. We then use the `normal()` function to simulate normal random variables. This function is in the [`random` sub-module](https://numpy.org/doc/stable/reference/random/index.html?highlight=random#module-numpy.random), which is a sub-module in `numpy` that collects all of the pseudorandom number generation functionality together.   

```{python, collapse = TRUE}
import numpy as np # import all of numpy
np.random.normal(size=10)
```

This is one use of the dot operator (`.`). We will also use it in TODO, but don't worry about that for now. `normal` is *inside of* `random`, which it itself inside of `np`. 

As a second example, suppose we were interested in the [`stats` sub-module](https://docs.scipy.org/doc/scipy/reference/tutorial/stats.html) found inside the `scipy` package. We could import all of `scipy`, but just like the above example, that would mean we would need to consistently refer to a variable's module, the sub-module, and the variable name. For long programs, this can become tedious if we had to type `scipy.stats.norm` over and over again. Instead, let's import the sub-module (or sub-package) and ignore the rest of `scipy`.


```{python, collapse = TRUE}
from scipy import stats
stats.norm().rvs(size=10)
```

So we don't have to type `scipy` every time we use something in `scipy.stats`.

Finally, we can import the function directly, and refer to it with only one letter. This is highly discouraged, though. We are much more likely to accidentally use the name `n` twice. Further, `n` is not a very descriptive name, which means it could be difficult to understand what your program is doing later. 

```{python, collapse = TRUE}
from numpy.random import normal as n
n(size=10)
```

Keep in mind, you're always at risk of accidentally re-using names, even if you aren't `import`ing anything. For example, consider the following code. 
```{python, eval = FALSE}
# don't do this!
sum = 3
```

This is very bad, because now you cannot use the `sum` function that was named in the built-in module. To see what is in your built in module, type the following into your Python interpreter: `dir(__builtins__)`.


## Control Flow

### Conditional Logic 

We discussed boolean objects in \@ref(basic-types). We used these for

- counting up number of times a condition appeared, and 
- subsetting.

Another way to use them is to conditionally execute code, depending on whether or truth condition of a boolean. 

In R, 
```{r, collapse = TRUE}
myName <- "Clare"
if(myName != "Taylor"){
    print("you are not Taylor")
}
```

[In Python](https://docs.python.org/3/tutorial/controlflow.html#if-statements), you don't need curly braces, but the indentation needs to be just right, and you need a colon.

```{python, collapse = TRUE}
my_name = "Taylor"
if my_name == "Taylor":
    print("hi Taylor")
```

There can be more than one truth test. To test alternative boolean conditions, you can add one or more `else if` (in R) or `elif` (in Python) blocks. The first block with a boolean that is found to be true will execute, and none of the resulting conditions will be checked.  

If no `if` block or `else if`/`elif` block executes, an `else` block will always execute. That's why `else` blocks don't need to look at a boolean. Whether they execute only depends on the booleans in the previous blocks.

```{r, collapse = TRUE}
food <- "muffin"
if(food == "apple"){
    print("an apple a day keeps the doctor away")
}else if(food == "muffin"){
    print("muffins have a lot of sugar in them")
}else{
    print("neither an apple nor a muffin")
}
```

```{python, collapse = TRUE}
my_num = 42.999
if my_num % 2 == 0:
    print("my_num is even")
elif my_num % 2 == 1:
    my_num += 1
    print("my_num was made even")
else:
    print("you're cheating by not using integers!")
```

### Loops

One line of code generally does one "thing," unless you're using loops. Code written inside a loop will execute many times. 

The most common loop for us will be a `for` loop. A simple `for` loop in R might look like this

```{r, collapse = TRUE}
myLength <- 9
r <- vector(mode = "numeric", length = myLength)
for(i in seq_len(myLength)){
    r[i] <- i
}
r
```

1. `seq_len(myLength)` gives us a `vector`
2. `i` is a variable that takes on the values found in the `vector`
3. Code inside the loop (inside the curly braces), is repeatedly executed, and it may or may not reference the dynamic variable `i`

[In Python](https://docs.python.org/3/tutorial/controlflow.html#for-statements)

```{python, collapse = TRUE}
my_length = 9
r = []
for i in range(my_length):
    r.append(i)
r
```

1. Unsurprisingly, Python's syntax opts for indentation and colons instead of curly braces and parentheses,
2. Code inside the loop (inside the curly braces), is repeatedly executed, and it may or may not reference the dynamic variable `i`
3. `for` loops in Python are more flexible because they iterate over many different types of data structures,
4. The [`range`](https://docs.python.org/3/tutorial/controlflow.html#the-range-function) doesn't generate all the numbers in the sequence at once, so it saves on memory. This can be quite useful for certain applications. However, `r` is a list that *does* store all the consecutive integers.

```{block, type='rmd-caution'}
Loop tips:

1. If you find yourself copy/paste-ing code, maybe changing a small portion on each line, you should consider using a loop, 
2. If a `for` loop works for something you're trying to do, first try to find a replacement function that does what you want. The examples above just made a `vector`/`list` of consecutive integers. There are many built in functions that accomplish this. Avoiding loops in this case would make your program shorter, easier to read, and (probably much) faster. Also check out `apply`-ish functions/methods (TODO link)
3. Watch out for [**off-by-one** errors](https://en.wikipedia.org/wiki/Off-by-one_error). Iterating over the wrong sequence is a common mistake, considering 
    * Python starts counting from $0$, while R starts counting from $1$
    * sometimes iteration `i` references the `i-1`th element of a container
    * The behavior of loops is sometimes more difficult to understand if they're using `break` or `continue`/`next` statements
4. Don't hardcode variables. In the example above, the  `myLength` or `my_length` variable could be referenced in many places throughout the entire program. If you wanted to change the number of iterations in your program (which happens all the time), and you *did* hardcode the length in a bunch of places throughout the program, you would need to hunt down all those changes. Be kind to yourself and don't give yourself extra work! 
```

Python provides an alternative way to construct lists similar to the one we constructed in the above example. They are called [**list comprehensions**](https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions). You can incorporate iteration and conditional logic in one line of code. 

```{python, collapse = TRUE}
[3*i for i in range(10) if i%2 == 0]
```
You might also have a look at [*generator expressions*](https://www.python.org/dev/peps/pep-0289/) and [*dictionary comprehensions*](https://www.python.org/dev/peps/pep-0274/).

R can come close to replicating the above behavior with vectorization, but the conditional part is hard to achieve without subsetting.

```{r, collapse = TRUE}
3*seq(0,9)[seq(0,9)%%2 == 0]
```




### A Longer Example

#### Description of Accept-Reject Sampling

An example of an algorithm that uses conditional logic is the **accept-reject sampling method** (TODO cite). This is useful for when we want to sample from a *target probability density* $p(x)$, using another distribution called a *proposal ditribution* $q(x)$. 

$q(x)$ is probably a distribution that is easy to sample from and is easy to evaluate pointwise. For example, a uniform distribution satisfies these criteria because both R and Python have functions that accomplish these two things (e.g. sampling can be done with `runif` in R and `np.random.uniform` in Python). $p(x)$ is generally more "complicated." If it wasn't, we would try to find some built-in function for it. 

One common way a distribution can be complicated is that it can have an unknown **normalizing constant**--one that is difficult or impossible to solve using calculus. This happens a lot in *Bayesian Statistics*, for example.^[The posterior distribution is usually the object of interest in Bayesian statistics. According to Bayes' Rule, the unnormalized posterior is usually the product of two "easy" functions. However, integrating the product is not always possible!]. We might write down 
$$
p(x) = \frac{f(x)}{\int f(x) dx},
$$
and this is guaranteed to be a probability density function as long as $f(x) \ge 0$  and $\int f(x) dx < \infty$, but we might have no idea how to solve the denominator. In this case, $f(x)$ is easy to evaluate pointwise, but $p(x)$ is not. 

This algorithm makes use of an auxiliary random variable that is sampled from a $\text{Bernoulli}(p)$ distribution. As long as $0 < p < 1$, a Bernoulli random variable $Y$ is either $0$ or $1$. The probability it takes the value $1$ is $p$, while the probability that it takes the value $0$ is $1-p$. A coin flip is a good example use-case for this distribution. Coin flips are commonly assumed to be distributed as$\text{Bernoulli}(.5)$. At least for fair coins, there is an equal chance that the coin lands heads (i.e. $0$) or tails (i.e. $1$).

The most difficult part about using this algorithm is that one must calculate the probability parameter of this Bernoulli random variable. This involves calculating (by hand) an upper bound $M$ for the ratio $f(x) / q(x)$. This bound has to hold uniformly, meaning that it is a constant number that is greater than the ratio no matter what $x$ we plug in. 

Below is one step of the accept-reject algorithm. 

******
**Algorithm 1**:  Accept-Reject Sampling (One Step)

******
1. Calculate $M > \frac{f(x)}{q(x)}$ (the smaller the better)
2.  Sample $X$ from $q(x)$
3.  Sample $Y \mid X$ from $\text{Bernoulli}\left(\frac{f(X)}{q(X)M}\right)$
4. If $Y = 1$, then return $X$
5. Otherwise, return nothing

******

Multiple samples will be required, so this process needs to be iterated many times. There are two ways to do this. If you want to iterate a fixed number of times, you can use a `for` loop. However, in that case, you will end up with a random number of samples. On the other hand, if you want a nonrandom number of samples, you will probably want a `while` loop. This is the approach the example below takes. The `while` loop will continue iterating until a condition is false. In our case, we want to loop until we receive the total number of samples we requested. 

#### A Specific Example

Here is a specific example. Let's say our target^[This is the density of a $\text{Beta}(3,2)$ random variable, if you're curious.] is 
$$
p(x) = 
\begin{cases}
\frac{x^2(1-x)}{\int_0^1 x^2(1-x) dx} & 0 < x < 1 \\
0 & \text{otherwise}
\end{cases}.
$$
The denominator, $\int_0^1 x^2(1-x) dx$, is the target's normalizing constant. You might know how to solve this integral, but let's pretend for the sake of our example that it's too difficult for us. We want to sample from $p(x)$ while only being able to evaluate (not sample) from its normalized version.

Next, let's choose a uniform distribution for our proposal distribution: 
$$
q(x) = 
\begin{cases}
1 & 0 < x < 1 \\
0 & \text{otherwise}
\end{cases}
$$
We can plot all three functions.

```{r, echo=F, out.width='80%'}
xGrid <- seq(0,1,by=.01)
f <- function(x) x^2*(1-x)
plot(xGrid, dbeta(xGrid,3,2), ylab = "", xlab = "x", type = "l", col = "black")
lines(xGrid, f(xGrid), col = "red")
lines(xGrid, rep(1, length(xGrid)), col = "green")
legend("topright", c("p(x)","f(x)", "q(x)"), col = c("black", "red", "green"), pch = 4)
```

Here's some Python code that attempts to sample once from $p(x)$. Sometimes proposals are not accepted. When that happens, the function returns `None`.

```{python, collapse = TRUE}
import numpy as np

def f(samp):
    """the unnormalized density"""
    return (1-samp)*(samp**2)

def attempt_one_samp():
    """attempts to sample from target distribution, using uniform as a proposal"""
    x = np.random.uniform()
    M = 4/27
    bern_prob_param = f(x)/M
    accept = np.random.binomial(1, bern_prob_param) == 1
    if accept:
        return x
```

```{python, collapse = TRUE}
def sample_from_target(num_times):
    """sample num_times from the target distribution"""
    samps = []
    while len(samps) < num_times:
        potential_samp = attempt_one_samp()
        if potential_samp is not None:
            samps.append(potential_samp)
    return samps
```


1. we used a `while` loop instead of a `for` loop because we did not know how many iterations it would take to get `num_times` samples
2. We are following the [Python style guide](https://www.python.org/dev/peps/pep-0008/#other-recommendations) and using the `is not` keyword to check if something is `None` 


In chapter TODO, we'll show you the code that you can use to generate the plot below. 
```{r, collapse = TRUE, echo=F}
samps <- rbeta(10000,3,2)
xGrid <- seq(min(samps), max(samps), length.out = 100)
hist(samps, main = "", xlab = "x", probability = T)
lines(xGrid, dbeta(xGrid,3,2), type = "l", col = "red")
```


Python:
if 
if/else
if/elif/else
for loops
list comprehensions



## Reshaping and Combining Data Sets


### Ordering and Sorting Data

Sorting a data set, in ascending order, say, is a common task. You might need to do it because 

1. ordering and ranking is commonly done in *nonparametric statistics*, 
2. you want to inspect the most "extreme" observations in a data set, 
3. it's a pre-processing step before generating visualizations.

In R, it all starts with `vector`s. There are two common functions you should know: `sort` and `order`. `sort` returns the sorted *data*, while `order` returns the *order indexes*. 

```{r, collapse = TRUE}
sillyData <- rnorm(5)
print(sillyData)
sort(sillyData)
order(sillyData)
```

`order` is useful if you're sorting a data frame by a particularly column. Below, we inspect the top 5 most expensive cars. Notice that we need to clean up the `MSRP` (a `character` vector) a little first. We use the function `gsub` to find patterns in the text, and replace them with the empty string. 

```{r}
carData <- read.csv("data/cars.csv")
noDollarSignMSRP <- gsub("$", "", carData$MSRP, fixed = TRUE)
carData$cleanMSRP <- as.numeric(gsub(",", "", noDollarSignMSRP, fixed = TRUE))
rowIndices <- order(carData$cleanMSRP, decreasing = TRUE)[1:5]
carData[rowIndices,c("Make", "Model", "MSRP", "cleanMSRP")]
```

In Python, Numpy has [`np.argsort`](https://numpy.org/doc/stable/reference/generated/numpy.argsort.html) and [`np.sort`](https://numpy.org/doc/stable/reference/generated/numpy.sort.html).

```{python, collapse = TRUE}
import numpy as np
silly_data = np.random.normal(size=5)
print(silly_data)
np.sort(silly_data)
np.argsort(silly_data)
```

For Pandas' `DataFrame`s, most of the functions I find useful are methods attached to the `DataFrame` class. That means that, as long as something is inside a `DataFrame`, you can use dot notation. This is discussed more in TODO.

```{python, collapse = TRUE}
import pandas as pd
car_data = pd.read_csv("data/cars.csv")
car_data['no_dlr_msrp'] = car_data['MSRP'].str.replace("$", "", regex = False)
car_data['clean_MSRP'] = car_data['no_dlr_msrp'].str.replace(",","").astype(float)
car_data = car_data.sort_values(by='clean_MSRP', ascending = False)
car_data[["Make", "Model", "MSRP", "clean_MSRP"]].head(5)
```

Pandas `DataFrame`s and `Series` have a [`replace`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.replace.html) method. We use this to remove dollar signs and commas from the MSRP column. Note that we had to access the `.str` attribute of the `Series` column before we used it. After the string was processed, we converted it to a `Series` of `float`s with the `astype` method. 

Finally, sorting the overall data frame could have been done with the same approach as the code we used in R (i.e. raw subsetting by row indexes), but there is a built in method called `sort_values` that will do it for us. 


### Stacking Data Sets and Placing them Shoulder to Shoulder


Stacking data sets on top of each other is a common task. You might need to do it if 

1. you need to add new a new row (or many rows) to a data frame, 
2. you need to recombine data sets (e.g. recombine a train/test split), or 
3. you're creating a matrix in a step-by-step way. 

In R, this can be done with `rbind` (short for "row bind")

```{r, collapse = TRUE}
realEstate <- read.csv("data/albemarle_real_estate.csv")
train <- realEstate[-1,]
test <- realEstate[1,]
head(rbind(test, train))
sum(rbind(test, train) != realEstate)
```

The above example was with `data.frame`s. This example of `rbind` is with `matrix` objects.

```{r, collapse=TRUE}
rbind(matrix(1,nrow = 2, ncol = 3), 
      matrix(2,nrow = 2, ncol = 3))
```


In Python, you can stack data frames with [`pd.concat`](https://www.google.com/search?client=safari&rls=en&q=pandas+concat&ie=UTF-8&oe=UTF-8). It has a lot of options, so feel free to peruse those. You can also replace the call to `pd.concat` below with [`test.append(train)`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.append.html). 


```{python, collapse = TRUE}
import pandas as pd
real_estate = pd.read_csv("data/albemarle_real_estate.csv")
train = real_estate.iloc[1:,]
test = real_estate.iloc[[0],] # need the extra brackets!
pd.concat([test,train], axis=0).head() # also 
(pd.concat([test,train], axis=0) != real_estate).sum().sum()
```

Take note of the extra square brackets when we create `test`. If you use `real_estate.iloc[0,]` instead, it will return a `Series` with all the elements coerced to the same type, and this won't `pd.concat` properly with the rest of the data! 

### Merging or Joining Data Sets


If you have two different data sets that provide different information about the same things, you put them together using a **`merge`** (aka **`join`**) statement. The resulting data set is wider, and possibly with fewer rows. In R, you can use the [`merge` function](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/merge). In Python, you can use the [`merge` method](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html#pandas-dataframe-merge).

Suppose you have to sets of supposedly anonymized data about individual accounts on some online platforms. 

```{r, collapse = TRUE}
baby1 <- read.csv("data/baby1.csv", stringsAsFactors = FALSE)
baby2 <- read.csv("data/baby2.csv", stringsAsFactors = FALSE)
head(baby1)
head(baby2)
```

```{python, echo = FALSE}
import pandas as pd
baby1 = pd.read_csv("data/baby1.csv")
baby2 = pd.read_csv("data/baby2.csv")
```

The first thing you need to ask yourself is "which column is the unique identifier that is shared between these two data sets?" In our case, they both have an "identification number" column, could that be it? Let's suppose for the sake of argument that these two data sets are coming from different online platforms, and these two places use different schemes to number their users. 

In this case, they both share a column with (possibly) the same information about email addresses. They are named differently in each data set, so we must specify both column names.

```{r, collapse = TRUE}
# in R
merge(baby1, baby2, by.x = "email_address", by.y = "email")
```

In Python, `merge` is a method attached to each `DataFrame` instance.

```{python, collapse = TRUE}
# in Python
baby1.merge(baby2, left_on = "email_address", right_on = "email")
```

The email addresses `anotherfake@gmail.com` and `notreal@gmail.com` exist in both data sets, so each of these email addresses will end up in the result data frame. The rows in the result data set are wider and have more attributes for each individual. 

Notice the duplicate email address, too. In this case, either the user signed up for two accounts using the same email, or one person signed up for an account with another person's email address. In the case of duplicates, both rows will match with the same rows in the other data frame. 

Also, in this case, all email addresses that weren't found in both data sets were thrown away. This does not necessarily need to be the intended behavior. For instance,  if we wanted to make sure no rows were thrown away, that would be possible. In this case, though, for email addresses that weren't found in both data sets, some information will be missing. Recall that Python and R handle missing data differently (see \@ref(how-do-r-and-python-handle-missing-values)).


```{r, collapse = TRUE}
# in R
merge(baby1, baby2, 
      by.x = "email_address", by.y = "email", 
      all.x = TRUE, all.y = TRUE)
```


```{python, collapse = TRUE}
# in Python
baby1.merge(baby2, 
            left_on = "email_address", right_on = "email", 
            how = "outer")
```

You can see it's slightly more concise in Python. If you are familiar with SQL, you might have heard of inner and outer joins. This is where Pandas [takes some of its argument names from](https://pandas.pydata.org/pandas-docs/version/0.15/merging.html#database-style-dataframe-joining-merging). 

### Long Versus Wide Data

TODO

## Visualization

### Base R Plotting

R comes with some built-in functions `plot`, `hist`, `boxplot`, etc. Many of these reside in `package:graphics`, which comes pre-loaded into the search path. `plot` on the other hand, is higher up the search path in `package:base`--it is a generic method whose methods might be in `package:graphics` or some place else. 

Base plotting covers most needs, so that's what we spend most time with. However, there are a large number of third-party libraries for plotting that you might consider looking into if you want to follow a certain aesthetic, or if you want plotting specialized for certain cases (e.g. geospatial plots).

Recall our Albemarle Real Estate data set. 

```{r, collapse = TRUE}
df <- read.csv("data/albemarle_real_estate.csv")
head(df)
```

If we wanted to get a general idea of how expensive homes were in Albemarle County, we could use a histogram. This helps us visualize a univariate numerical variable/column. Below I plot the (natural) logarithm of home prices. 

```{r, collapse=TRUE}
hist(log(df$TotalValue), 
     xlab = "natural logarithm of home price", main = "Super-Duper Plot!")
```

I specified the `xlab=` and `main=` arguments, but there are many more that could be tweaked. Make sure to skim the options in the documentation (`?hist`).

`plot` is useful for plotting two univariate numerical variables. This can be done in time series plots (variable versus time) and scatter plots (one variable versus another).

```{r, collapse=T}
par(mfrow=c(1,2))
plot(df$TotalValue, df$LotSize, 
     xlab = "total value ($)", ylab = "lot size (sq. ft.)",
     pch = 3, col = "red", type = "b")
plot(log(df$TotalValue), log(df$LotSize), 
     xlab = "log. total value", ylab = "log. lot size", 
     pch = 2, col = "blue", type = "p")
abline(h = log(mean(df$LotSize)), col = "green")
par(mfrow=c(1,1))
```

I use some of the many arguments available (type `?plot`). `xlab=` and `ylab=` specify the x- and y-axis labels, respectively. `col=` is short for "color." `pch=` is short for "point character." Changing this will change the symbol shapes used for each point. `type=` is more general than that, but it is related. I typically use it to specify whether or not I want the points connected with lines.

I also use a couple other functions. `abline` is used to superimpose lines over the top of a plot. They can be `h`orizontal, `v`ertical, or you can specify them in slope-intercept form, or by providing a linear model object. I also used `par` to set a graphical parameter. The graphical parameter `par()$mfrow` sets the layout of a multiple plot visualization. I then set it back to the standard $1 \times 1$ layout afterwards.


### Plotting with Matplotlib


## Exercises



1. Write a Metropolis-Hastings algorithm.

2. Make a gif

3. What kind of situation would a left- or right-join be used? 

4. Make a "phase plot"