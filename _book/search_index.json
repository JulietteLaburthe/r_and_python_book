[["reshaping-and-combining-data-sets.html", "Chapter 10 Reshaping and Combining Data Sets 10.1 Ordering and Sorting Data 10.2 Stacking Data Sets and Placing Them Shoulder to Shoulder 10.3 Merging or Joining Data Sets 10.4 Long Versus Wide Data", " Chapter 10 Reshaping and Combining Data Sets 10.1 Ordering and Sorting Data Sorting a data set, in ascending order, say, is a common task. You might need to do it because ordering and ranking is commonly done in nonparametric statistics, you want to inspect the most “extreme” observations in a data set, it’s a pre-processing step before generating visualizations. In R, it all starts with vectors. There are two common functions you should know: sort() and order(). sort() returns the sorted data, while order() returns the order indexes. sillyData &lt;- rnorm(5) print(sillyData) ## [1] -0.1693271 -2.0247190 -0.6156624 0.8463154 0.5502179 sort(sillyData) ## [1] -2.0247190 -0.6156624 -0.1693271 0.5502179 0.8463154 order(sillyData) ## [1] 2 3 1 5 4 order() is useful if you’re sorting a data frame by a particularly column. Below, we inspect the top 5 most expensive cars. Notice that we need to clean up the MSRP (a character vector) a little first. We use the function gsub() to find patterns in the text, and replace them with the empty string. carData &lt;- read.csv(&quot;data/cars.csv&quot;) noDollarSignMSRP &lt;- gsub(&quot;$&quot;, &quot;&quot;, carData$MSRP, fixed = TRUE) carData$cleanMSRP &lt;- as.numeric(gsub(&quot;,&quot;, &quot;&quot;, noDollarSignMSRP, fixed = TRUE)) rowIndices &lt;- order(carData$cleanMSRP, decreasing = TRUE)[1:5] carData[rowIndices,c(&quot;Make&quot;, &quot;Model&quot;, &quot;MSRP&quot;, &quot;cleanMSRP&quot;)] ## Make Model MSRP cleanMSRP ## 335 Porsche 911 GT2 2dr $192,465 192465 ## 263 Mercedes-Benz CL600 2dr $128,420 128420 ## 272 Mercedes-Benz SL600 convertible 2dr $126,670 126670 ## 271 Mercedes-Benz SL55 AMG 2dr $121,770 121770 ## 262 Mercedes-Benz CL500 2dr $94,820 94820 In Python, Numpy has np.argsort() and np.sort(). import numpy as np silly_data = np.random.normal(size=5) print(silly_data) ## [ 7.81504484e-04 -8.49078814e-01 6.21304393e-01 -1.76167440e+00 ## -1.37068606e-01] np.sort(silly_data) ## array([-1.76167440e+00, -8.49078814e-01, -1.37068606e-01, 7.81504484e-04, ## 6.21304393e-01]) np.argsort(silly_data) ## array([3, 1, 4, 0, 2]) For Pandas’ DataFrames, most of the functions I find useful are methods attached to the DataFrame class. That means that, as long as something is inside a DataFrame, you can use dot notation. import pandas as pd car_data = pd.read_csv(&quot;data/cars.csv&quot;) car_data[&#39;no_dlr_msrp&#39;] = car_data[&#39;MSRP&#39;].str.replace(&quot;$&quot;, &quot;&quot;, regex = False) car_data[&#39;clean_MSRP&#39;] = car_data[&#39;no_dlr_msrp&#39;].str.replace(&quot;,&quot;,&quot;&quot;).astype(float) car_data = car_data.sort_values(by=&#39;clean_MSRP&#39;, ascending = False) car_data[[&quot;Make&quot;, &quot;Model&quot;, &quot;MSRP&quot;, &quot;clean_MSRP&quot;]].head(5) ## Make Model MSRP clean_MSRP ## 334 Porsche 911 GT2 2dr $192,465 192465.0 ## 262 Mercedes-Benz CL600 2dr $128,420 128420.0 ## 271 Mercedes-Benz SL600 convertible 2dr $126,670 126670.0 ## 270 Mercedes-Benz SL55 AMG 2dr $121,770 121770.0 ## 261 Mercedes-Benz CL500 2dr $94,820 94820.0 Pandas’ DataFrames and Series have a .replace() method. We use this to remove dollar signs and commas from the MSRP column. Note that we had to access the .str attribute of the Series column before we used it. After the string was processed, we converted it to a Series of floats with the astype() method. Finally, sorting the overall data frame could have been done with the same approach as the code we used in R (i.e. raw subsetting by row indexes), but there is a built-in method called sort_values() that will do it for us. 10.2 Stacking Data Sets and Placing Them Shoulder to Shoulder Stacking data sets on top of each other is a common task. You might need to do it if you need to add new a new row (or many rows) to a data frame, you need to recombine data sets (e.g. recombine a train/test split), or you’re creating a matrix in a step-by-step way. In R, this can be done with rbind() (short for “row bind”) realEstate &lt;- read.csv(&quot;data/albemarle_real_estate.csv&quot;) train &lt;- realEstate[-1,] test &lt;- realEstate[1,] head(rbind(test, train)) ## YearBuilt YearRemodeled Condition NumStories FinSqFt Bedroom FullBath ## 1 2006 0 Average 1.00 1922 3 3 ## 2 2003 0 Average 1.00 1848 3 2 ## 3 1972 0 Average 1.00 1248 2 1 ## 4 1998 0 Good 1.00 1244 1 1 ## 5 1886 0 Average 1.86 1861 4 1 ## 6 1910 0 Fair 1.53 1108 3 1 ## HalfBath TotalRooms LotSize TotalValue City ## 1 0 10 5.000 409900 CROZET ## 2 0 7 61.189 523100 CROZET ## 3 0 4 1.760 180900 EARLYSVILLE ## 4 0 3 50.648 620700 CROZET ## 5 0 6 3.880 162500 CROZET ## 6 0 6 8.838 167200 CROZET sum(rbind(test, train) != realEstate) ## [1] 0 The above example was with data.frames. This example of rbind() is with matrix objects. rbind(matrix(1,nrow = 2, ncol = 3), matrix(2,nrow = 2, ncol = 3)) ## [,1] [,2] [,3] ## [1,] 1 1 1 ## [2,] 1 1 1 ## [3,] 2 2 2 ## [4,] 2 2 2 In Python, you can stack data frames with pd.concat(). It has a lot of options, so feel free to peruse those. You can also replace the call to pd.concat() below with test.append(train). import pandas as pd real_estate = pd.read_csv(&quot;data/albemarle_real_estate.csv&quot;) train = real_estate.iloc[1:,] test = real_estate.iloc[[0],] # need the extra brackets! pd.concat([test,train], axis=0).head() # also ## YearBuilt YearRemodeled Condition ... LotSize TotalValue City ## 0 2006 0 Average ... 5.000 409900 CROZET ## 1 2003 0 Average ... 61.189 523100 CROZET ## 2 1972 0 Average ... 1.760 180900 EARLYSVILLE ## 3 1998 0 Good ... 50.648 620700 CROZET ## 4 1886 0 Average ... 3.880 162500 CROZET ## ## [5 rows x 12 columns] (pd.concat([test,train], axis=0) != real_estate).sum().sum() ## 0 Take note of the extra square brackets when we create test. If you use real_estate.iloc[0,] instead, it will return a Series with all the elements coerced to the same type, and this won’t pd.concat() properly with the rest of the data! 10.3 Merging or Joining Data Sets If you have two different data sets that provide different information about the same things, you put them together using a merge (aka join) statement. The resulting data set is wider, and it may end up with either more or fewer rows. In R, you can use the merge() function. In Python, you can use the .merge() method. Suppose you have to sets of supposedly anonymized data about individual accounts on some online platforms. # in R baby1 &lt;- read.csv(&quot;data/baby1.csv&quot;, stringsAsFactors = FALSE) baby2 &lt;- read.csv(&quot;data/baby2.csv&quot;, stringsAsFactors = FALSE) head(baby1) ## idnum height.inches. email_address ## 1 1 74 fakeemail123@gmail.com ## 2 3 66 anotherfake@gmail.com ## 3 4 62 notreal@gmail.com ## 4 23 62 notreal@gmail.com head(baby2) ## idnum phone email ## 1 3901283 5051234567 notreal@gmail.com ## 2 41823 5051234568 notrealeither@gmail.com ## 3 7198273 5051234568 anotherfake@gmail.com The first thing you need to ask yourself is “which column is the unique identifier that is shared between these two data sets?” In our case, they both have an “identification number” column. However, these two data sets are coming from different online platforms, and these two places use different schemes to number their users. In this case, it is better to merge on the email addresses. Users might be using different email addresses on these two platforms, but there’s a stronger guarantee that matched email addresses means that you’re matching the right accounts. The columns are named differently in each data set, so we must specify them by name. # in R merge(baby1, baby2, by.x = &quot;email_address&quot;, by.y = &quot;email&quot;) ## email_address idnum.x height.inches. idnum.y phone ## 1 anotherfake@gmail.com 3 66 7198273 5051234568 ## 2 notreal@gmail.com 4 62 3901283 5051234567 ## 3 notreal@gmail.com 23 62 3901283 5051234567 In Python, merge() is a method attached to each DataFrame instance. # in Python baby1.merge(baby2, left_on = &quot;email_address&quot;, right_on = &quot;email&quot;) ## idnum_x height(inches) ... phone email ## 0 3 66 ... 5051234568 anotherfake@gmail.com ## 1 4 62 ... 5051234567 notreal@gmail.com ## 2 23 62 ... 5051234567 notreal@gmail.com ## ## [3 rows x 6 columns] The email addresses anotherfake@gmail.com and notreal@gmail.com exist in both data sets, so each of these email addresses will end up in the result data frame. The rows in the result data set are wider and have more attributes for each individual. Notice the duplicate email address, too. In this case, either the user signed up for two accounts using the same email, or one person signed up for an account with another person’s email address. In the case of duplicates, both rows will match with the same rows in the other data frame. Also, in this case, all email addresses that weren’t found in both data sets were thrown away. This does not necessarily need to be the intended behavior. For instance, if we wanted to make sure no rows were thrown away, that would be possible. In this case, though, for email addresses that weren’t found in both data sets, some information will be missing. Recall that Python and R handle missing data differently (see 2.8.2). # in R merge(baby1, baby2, by.x = &quot;email_address&quot;, by.y = &quot;email&quot;, all.x = TRUE, all.y = TRUE) ## email_address idnum.x height.inches. idnum.y phone ## 1 anotherfake@gmail.com 3 66 7198273 5051234568 ## 2 fakeemail123@gmail.com 1 74 NA NA ## 3 notreal@gmail.com 4 62 3901283 5051234567 ## 4 notreal@gmail.com 23 62 3901283 5051234567 ## 5 notrealeither@gmail.com NA NA 41823 5051234568 # in Python baby1.merge(baby2, left_on = &quot;email_address&quot;, right_on = &quot;email&quot;, how = &quot;outer&quot;) ## idnum_x height(inches) ... phone email ## 0 1.0 74.0 ... NaN NaN ## 1 3.0 66.0 ... 5.051235e+09 anotherfake@gmail.com ## 2 4.0 62.0 ... 5.051235e+09 notreal@gmail.com ## 3 23.0 62.0 ... 5.051235e+09 notreal@gmail.com ## 4 NaN NaN ... 5.051235e+09 notrealeither@gmail.com ## ## [5 rows x 6 columns] You can see it’s slightly more concise in Python. If you are familiar with SQL, you might have heard of inner and outer joins. This is where pandas takes some of its argument names from. Finally, if both data sets have multiple values in the column you’re joining on, the result can have more rows than either table. This is because every possible match shows up. # in R first &lt;- data.frame(category = c(&#39;a&#39;,&#39;a&#39;), measurement = c(1,2)) merge(first, first, by.x = &quot;category&quot;, by.y = &quot;category&quot;) ## category measurement.x measurement.y ## 1 a 1 1 ## 2 a 1 2 ## 3 a 2 1 ## 4 a 2 2 # in Python first = pd.DataFrame({&#39;category&#39; : [&#39;a&#39;,&#39;a&#39;], &#39;measurement&#39; : [1,2]}) first.merge(first, left_on = &quot;category&quot;, right_on = &quot;category&quot;) ## category measurement_x measurement_y ## 0 a 1 1 ## 1 a 1 2 ## 2 a 2 1 ## 3 a 2 2 10.4 Long Versus Wide Data 10.4.1 Long Versus Wide in R Many types of data can be stored in either a wide or long format. The classical example is data from a longitudinal study. If an experimental unit (in the example below this is a person) is repeatedly measured over time, each row would correspond to an experimental unit and an observation time in a data set in a long form. fakeLongData1 &lt;- data.frame(person = c(&quot;Taylor&quot;,&quot;Taylor&quot;,&quot;Charlie&quot;,&quot;Charlie&quot;), timeObserved = c(1, 2, 1, 2), nums = c(100,101,300,301)) fakeLongData1 ## person timeObserved nums ## 1 Taylor 1 100 ## 2 Taylor 2 101 ## 3 Charlie 1 300 ## 4 Charlie 2 301 A long format can also be used if you have multiple observations (at a single time point) on an experimental unit. Here is another example. fakeLongData2 &lt;- data.frame(person = c(&quot;Taylor&quot;, &quot;Taylor&quot;, &quot;Charlie&quot;,&quot;Charlie&quot;), attributeName = c(&quot;attrA&quot;,&quot;attrB&quot;,&quot;attrA&quot;,&quot;attrB&quot;), nums = c(100,101,300,301)) fakeLongData2 ## person attributeName nums ## 1 Taylor attrA 100 ## 2 Taylor attrB 101 ## 3 Charlie attrA 300 ## 4 Charlie attrB 301 If you would like to reshape the long data sets into a wide format, you can use the reshape() function. You will need to specify which columns correspond with the experimental unit, and which column is the “factor” variable. fakeWideData1 &lt;- reshape(fakeLongData1, direction = &quot;wide&quot;, timevar = &quot;timeObserved&quot;, idvar = &quot;person&quot;, varying = c(&quot;before&quot;,&quot;after&quot;)) # col names in new data set fakeLongData1 ## person timeObserved nums ## 1 Taylor 1 100 ## 2 Taylor 2 101 ## 3 Charlie 1 300 ## 4 Charlie 2 301 fakeWideData1 ## person before after ## 1 Taylor 100 101 ## 3 Charlie 300 301 fakeWideData2 &lt;- reshape(fakeLongData2, direction = &quot;wide&quot;, timevar = &quot;attributeName&quot;, # timevar is a misnomer here idvar = &quot;person&quot;, varying = c(&quot;attribute A&quot;,&quot;attribute B&quot;)) fakeLongData2 ## person attributeName nums ## 1 Taylor attrA 100 ## 2 Taylor attrB 101 ## 3 Charlie attrA 300 ## 4 Charlie attrB 301 fakeWideData2 ## person attribute A attribute B ## 1 Taylor 100 101 ## 3 Charlie 300 301 reshape() will also go in the other direction: it can take wide data and convert it into long data reshape(fakeWideData1, direction = &quot;long&quot;, idvar = &quot;person&quot;, varying = list(c(&quot;before&quot;,&quot;after&quot;)), v.names = &quot;nums&quot;) ## person time nums ## Taylor.1 Taylor 1 100 ## Charlie.1 Charlie 1 300 ## Taylor.2 Taylor 2 101 ## Charlie.2 Charlie 2 301 fakeLongData1 ## person timeObserved nums ## 1 Taylor 1 100 ## 2 Taylor 2 101 ## 3 Charlie 1 300 ## 4 Charlie 2 301 reshape(fakeWideData2, direction = &quot;long&quot;, idvar = &quot;person&quot;, varying = list(c(&quot;attribute A&quot;,&quot;attribute B&quot;)), v.names = &quot;nums&quot;) ## person time nums ## Taylor.1 Taylor 1 100 ## Charlie.1 Charlie 1 300 ## Taylor.2 Taylor 2 101 ## Charlie.2 Charlie 2 301 fakeLongData2 ## person attributeName nums ## 1 Taylor attrA 100 ## 2 Taylor attrB 101 ## 3 Charlie attrA 300 ## 4 Charlie attrB 301 10.4.2 Long Versus Wide in Python With Pandas, we can take make wide data long with pd.DataFrame.pivot(), and we can go in the other direction with pd.DataFrame.melt(). When going from wide to long, make sure to use the pd.DataFrame.reset_index() method afterwards to reshape the data and remove the index. Here is an example similar to the one above. import pandas as pd fake_long_data1 = pd.DataFrame( {&#39;person&#39; : [&quot;Taylor&quot;,&quot;Taylor&quot;,&quot;Charlie&quot;,&quot;Charlie&quot;], &#39;time_observed&#39; : [1, 2, 1, 2], &#39;nums&#39; : [100,101,300,301]}) fake_long_data1 ## person time_observed nums ## 0 Taylor 1 100 ## 1 Taylor 2 101 ## 2 Charlie 1 300 ## 3 Charlie 2 301 pivot_data1 = fake_long_data1.pivot(index=&#39;person&#39;, columns=&#39;time_observed&#39;, values=&#39;nums&#39;) fake_wide_data1 = pivot_data1.reset_index() fake_wide_data1 ## time_observed person 1 2 ## 0 Charlie 300 301 ## 1 Taylor 100 101 Here’s one more example showing the same functionality–going from wide to long format. fake_long_data2 = pd.DataFrame({&#39;person&#39; : [&quot;Taylor&quot;,&quot;Taylor&quot;,&quot;Charlie&quot;,&quot;Charlie&quot;], &#39;attribute_name&#39; : [&#39;attrA&#39;, &#39;attrB&#39;, &#39;attrA&#39;, &#39;attrB&#39;], &#39;nums&#39; : [100,101,300,301]}) fake_wide_data2 = fake_long_data2.pivot(index=&#39;person&#39;, columns=&#39;attribute_name&#39;, values=&#39;nums&#39;).reset_index() fake_wide_data2 ## attribute_name person attrA attrB ## 0 Charlie 300 301 ## 1 Taylor 100 101 Here are some examples of going in the other direction: from wide to long with pd.DataFrame.melt(). The first example specifies value columns by integers. fake_wide_data1 ## time_observed person 1 2 ## 0 Charlie 300 301 ## 1 Taylor 100 101 fake_wide_data1.melt(id_vars = &quot;person&quot;, value_vars = [1,2]) ## person time_observed value ## 0 Charlie 1 300 ## 1 Taylor 1 100 ## 2 Charlie 2 301 ## 3 Taylor 2 101 The second example uses strings to specify value columns. fake_wide_data2 ## attribute_name person attrA attrB ## 0 Charlie 300 301 ## 1 Taylor 100 101 fake_wide_data2.melt(id_vars = &quot;person&quot;, value_vars = [&#39;attrA&#39;,&#39;attrB&#39;]) ## person attribute_name value ## 0 Charlie attrA 300 ## 1 Taylor attrA 100 ## 2 Charlie attrB 301 ## 3 Taylor attrB 101 TODO exercises just have them read in correctly Recall the file “long-beach-va.data” from UCI Machine Learning Repository. "]]
