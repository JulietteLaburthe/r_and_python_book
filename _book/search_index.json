[["functional-programming.html", "Chapter 5 Functional Programming 5.1 Background 5.2 Functions as Function Inputs in R 5.3 Another Example in R 5.4 Functions as Function Inputs in Base Python 5.5 Selected Numpy Functionals 5.6 Functional Methods in Pandas 5.7 Functions as Function Outputs in R 5.8 Functions as Function Outputs in Python", " Chapter 5 Functional Programming 5.1 Background Even though this book only discusses one of our languages of interest, the motivation for functional programming in both R and Python is clearly explained in Advanced R: A functional style tends to create functions that can easily be analysed in isolation (i.e. using only local information), and hence is often much easier to automatically optimise or parallelise. This sounds like a good thing to aspire to, so what is functional programming? Unfortunately, it’s hard to give a one-line definition that is also comprehensive. Even with a definition, it isn’t immediately obvious why the pieces of that definition would be conducive to the above goals. Here is my definition: a functional programming style is a style of programming that favors using functions that are both first-class and pure. Neither R nor Python is a 100% pure functional language. So for us, it’s a style that we can choose to let us guide us, or that we can disregard. You can choose to employ a more functional style, or you can choose to use elements discussed in 4. Some people unilaterally prefer one to the other, but others prefer to decide which to use depending on the task at hand. First class functions are functions that can be passed as arguments to other functions, can be returned from other functions, and can be assigned to variables or stored in data structures. TODO quote. Functions in both R and Python are first class. Many of the examples we discuss in this chapter would not exist if they were not. Pure functions return the same output if they are given the same input, and they do not produce side-effects. Side-effects are changes made to non-temporary variables, to the “state” of the program. If your function refers to or modifies global variables, or if it modifies arguments passed in, then it is more difficult to understand. As was discussed in TODO, this is possible in both R and Python, but the function is not completely understandable   a.) by itself in isolation, or   b.) without knowing how many times it is called. Regarding a.), we discussed examples where changing the value of a single global variable can completely change the behavior of an entire program (not just a single function!). Regarding b.) a variable has referential transparency if it can be replaced with a single value without changing the program’s behavior. One of the biggest tip-offs that you should be using functional programming is if you need to evaluate a single function many times, or in many different ways. This can happen in a great number of ways. Instead of copy/pasting similar-looking lines of code, you might consider higher-order functions that take your function as an input, and intelligently call it in all the many ways you want it to. A third option, not as heavily-discussed in this section, is to use a loop structure (c.f. 3.3.2). Another tip-off is if you need many different functions that are all resembling one another. Should you define each function separately, using excessive copy/paste-ing? Or should you write a function that can elegantly generate any function you need? 5.2 Functions as Function Inputs in R Many of the most commonly-used functionals in R have names that end in “apply.” The ones I discuss are sapply, vapply, lapply, apply, tapply and mapply. Each of these takes a function as one of its arguments. This is possible because R has first-class functions. Suppose we have a data.frame that has 10 rows and 100 columns. What if we want to take the mean of each column? An amateurish way to do this would be something like the following. myFirstMean &lt;- mean(myDF[,1]) mySecondMean &lt;- mean(myDF[,2]) myThirdMean &lt;- mean(myDF[,3]) # so on and so forth You need one line of code for each column in this data frame. For data frames with a lot of columns, this becomes quite tedious. You should also ask yourself what happens to you and your collaborators when the data frame changes even slightly, or if you want to apply a different function to its columns. Third, the results are not stored in a single container. You are making it difficult on yourself if you want to use these variables in subsequent pieces of code. TODO cite DRY Instead, prefer the use of sapply in this situation. The “s” in sapply stands for “simplified.” In this bit of code mean is called on each column of th data frame. sapply applies the function over columns, instead of rows, because data frames are internally a list of columns. myMeans &lt;- sapply(myDF, mean) head(myMeans) ## X1 X2 X3 X4 X5 X6 ## -0.501587168 0.566689611 -0.646456651 -0.003144197 0.009405446 -0.012763407 Each call to mean returns a double vector of length \\(1\\). This is necessary if you want to collect all the results into a vector–remember, all elements of a vector have to have the same type. To get the same behavior, you might also consider using vapply(myDF, mean, numeric(1)). In the above case, “simplify” referred to how one-hundred length-\\(1\\) vectors were simplified into one length-\\(100\\) vector. However, “simplified” does not necessarily imply that all elements will be stored in a vector. Consider the summary function, which returns a double vector of length \\(6\\). In this case, one-hundred length-\\(6\\) vectors were simplified into one \\(6 \\times 100\\) matrix. mySummaries &lt;- sapply(myDF, summary) is.matrix(mySummaries) ## [1] TRUE dim(mySummaries) ## [1] 6 100 Another function that is worth mentioning is replicate–it is a wrapper for sapply. Consider a situation where you want to call a function many times with the same inputs. You might try something like sapply(1:100, function(elem) { return(myFunc(someInput)) } ). Another, more readable, way to do this is replicate(100, myFunc(someInput)). For functions that do not return amenable types that fit into a vector, matrix or array, they might need to be stored in list. In this situation, you would need lapply. The “l” in lapply stands for “list.” lapply always returns a list of the same length as the input. regress &lt;- function(y){ lm(y ~ 1) } myRegs &lt;- lapply(myDF, regress) length(myRegs) ## [1] 100 class(myRegs[[1]]) ## [1] &quot;lm&quot; summary(myRegs[[12]]) ## ## Call: ## lm(formula = y ~ 1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.4771 -0.6266 0.1616 0.2547 1.2472 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.1730 0.2773 -0.624 0.548 ## ## Residual standard error: 0.877 on 9 degrees of freedom I use sapply and lapply the most, personally. The next most common function I use is apply. I use it to apply functions to rows instead of columns, although, it can also apply functions over columns, just as the other functions we discussed can. dim(myDF) ## [1] 10 100 apply(myDF, 1, mean) ## [1] -0.04074782 -0.05632855 0.05257684 -0.11985793 0.04400712 -0.08684332 ## [7] -0.03035439 0.12315101 -0.12723582 -0.07951232 tapply can be very handy when you need it. First, we’ve alluded to the definition before in TODO, but a ragged array is a collection of arrays that all have potentially different lengths. I don’t typically construct such an object and then pass it to tapply. Rather, I let tapply construct the ragged array for me. The first argument it expects is “typically vector-like,” while the second tells us how to break that vector into chunks. The third argument is a function that gets applied to each vector chunk. If I wanted the average home price for each city, I could use something like this. albRealEstate &lt;- read.csv(&quot;data/albemarle_real_estate.csv&quot;) head(albRealEstate) ## YearBuilt YearRemodeled Condition NumStories FinSqFt Bedroom FullBath ## 1 2006 0 Average 1.00 1922 3 3 ## 2 2003 0 Average 1.00 1848 3 2 ## 3 1972 0 Average 1.00 1248 2 1 ## 4 1998 0 Good 1.00 1244 1 1 ## 5 1886 0 Average 1.86 1861 4 1 ## 6 1910 0 Fair 1.53 1108 3 1 ## HalfBath TotalRooms LotSize TotalValue City ## 1 0 10 5.000 409900 CROZET ## 2 0 7 61.189 523100 CROZET ## 3 0 4 1.760 180900 EARLYSVILLE ## 4 0 3 50.648 620700 CROZET ## 5 0 6 3.880 162500 CROZET ## 6 0 6 8.838 167200 CROZET unique(albRealEstate$City) ## [1] &quot;CROZET&quot; &quot;EARLYSVILLE&quot; &quot;CHARLOTTESVILLE&quot; &quot;SCOTTSVILLE&quot; ## [5] &quot;NORTH GARDEN&quot; &quot;KESWICK&quot; tapply(albRealEstate$TotalValue, list(albRealEstate$City), mean) ## CHARLOTTESVILLE CROZET EARLYSVILLE KESWICK NORTH GARDEN ## 381933.0 380425.7 439141.0 540532.6 366597.8 ## SCOTTSVILLE ## 268407.4 You might be wondering why we put albRealEstate$City into a list. That seems kind of unnecessary. This is because tapply can be used with multiple factors–this will break down the vector input into a finer partition. The second argument must be one object, though, so all of these factors must be collected into a list. The following code produces a “pivot table.” tapply(albRealEstate$TotalValue, list(albRealEstate$City, albRealEstate$Condition), mean) ## Average Excellent Fair Good Poor Substandard ## CHARLOTTESVILLE 337913.4 491702.4 229336.2 444325.5 202420.00 457500 ## CROZET 342133.4 552081.6 198009.5 390657.7 203806.25 53450 ## EARLYSVILLE 365990.6 652387.4 230646.2 470554.7 372442.86 160400 ## KESWICK 392998.6 871719.8 172790.9 672510.7 100337.50 NA ## NORTH GARDEN 241187.7 966528.6 131997.0 440503.1 151187.50 NA ## SCOTTSVILLE 214046.9 502273.8 157438.0 374600.4 95377.78 NA For functions that return higher-dimensional output, you will have to use something like by or aggregate in place of tapply. The documentation of mapply states mapply is a multivariate version of sapply. sapply worked with univariate functions: the function was called multiple times, but each time with a single argument. If you have a function that takes multiple arguments, and you want those arguments to change each time the function is called, then you might be able to use mapply. Here is a short example. Regarding the n= argument of rnorm, the documentation explains, “[i]f length(n) &gt; 1, the length is taken to be the number required.” This would be a problem if we want to sample a.) three times from a mean \\(0\\) normal, b.) twice from a mean \\(100\\) normal, and c.) once from a mean \\(-100\\) normal distribution. rnorm(n = c(3,2,1), mean = c(0,100,-100), sd = c(.01, .01, .01)) ## [1] -0.01517804 100.00111662 -100.00185682 mapply(rnorm, n = c(3,2,1), mean = c(0,100,-100), sd = c(.01, .01, .01)) ## [[1]] ## [1] 0.005354238 -0.001131625 0.005523864 ## ## [[2]] ## [1] 100.0145 100.0128 ## ## [[3]] ## [1] -99.99725 I conclude by mentioning Reduce. Instead of applying a unary function to consecutive elements, it applies a binary function to elements. Reduce’s function argument gets called on the first two elements, then on the first output and the third element, then on the second output and fourth element, and so on. Here is an initial example. sum(1:3) ## [1] 6 Reduce(sum, 1:3) ## [1] 6 You can make the examples more interesting by making the binary function more complicated, or by changing the data structures. For instance, I often use Reduce for combining many data sets. myVecs &lt;- lapply(1:10, function(num) matrix(rep(num,4), ncol=1)) Reduce(cbind, myVecs) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 1 2 3 4 5 6 7 8 9 10 ## [2,] 1 2 3 4 5 6 7 8 9 10 ## [3,] 1 2 3 4 5 6 7 8 9 10 ## [4,] 1 2 3 4 5 6 7 8 9 10 Filter is useful with predicate functions. Predicate functions return a boolean. Repeatedly calling a predicate on different inputs of any container can be done as follows. TODO 5.3 Another Example in R Consider another less contrived example. Suppose we want to plot a bivariate Gaussian distribution. \\[ f(x,y) = \\frac{1}{2 \\pi} \\exp\\left[ -\\frac{x ^2 + y^2}{2} \\right] \\] The random elements \\(x\\) and \\(y\\), in this particular case, are uncorrelated, each have unit variance, and zero mean. This density is a surface in 3-d dimensional space. To visualize this, we would need to generate a “grid” of points in \\(\\mathbb{R}^2\\), evaluate our function on each point, and then call some plotting function that takes this all and makes a pretty picture. There are a couple ways you could write this function. One way might take two arguments, and another might take one argument. If we are to use mapply, we need the function to take two arguments. fTwoArgs &lt;- function(x,y){ exp(-.5*(x^2 + y^2)) / 2 / pi } We can construct every possible point on a grid with the expand.grid function. xGrid &lt;- seq(-3,3,.1) yGrid &lt;- seq(-3,3,.1) grid &lt;- expand.grid(xGrid, yGrid) head(grid) ## Var1 Var2 ## 1 -3.0 -3 ## 2 -2.9 -3 ## 3 -2.8 -3 ## 4 -2.7 -3 ## 5 -2.6 -3 ## 6 -2.5 -3 mapply would take fTwoArgs, and effectively call it on every row pair. The pairs do not need to be organized in a data.frame, though. funcOut1 &lt;- mapply(fTwoArgs, grid[,1], grid[,2]) head(funcOut1) ## [1] 1.964128e-05 2.638072e-05 3.508008e-05 4.618400e-05 6.019766e-05 ## [6] 7.768277e-05 rectangularOutput &lt;- matrix(funcOut1, ncol = length(xGrid)) persp(xGrid, yGrid, rectangularOutput, zlab = &quot;f(x,y)&quot;, xlab = &quot;x&quot;, ylab = &quot;y&quot;) If you prefer using apply, that is also possible, but you would need to rewrite the function to take one (length-two) argument. fOneArg &lt;- function(vec){ exp(-sum(vec^2)/2)/2/pi } funcOut2 &lt;- apply(grid, 1, fOneArg) moreRectOut &lt;- matrix(funcOut2, ncol = length(xGrid)) contour(xGrid, yGrid, moreRectOut, xlab = &quot;x&quot;, ylab = &quot;y&quot;) 5.4 Functions as Function Inputs in Base Python I discuss two functions from base Python that take functions as input. Neither return a list or a np.array, but they do return different kinds of iterables, which are “objects capable of returning their members one at a time,” according to the Python documentation. map, the function, will return objects of type map. filter, the function, will return objects of type filter. Often times we will just convert these to the container we are more familiar with. 5.4.1 map map can call a function repeatedly using elements of a container as inputs. Here is an example of calculating outputs of a spline function, which can be useful for coming up with predictors in regression models. This particular spline function is \\(f(x) = (x-k)1(x \\ge k)\\), where \\(k\\) is some chosen “knot point.” import numpy as np my_inputs = np.linspace(start = 0, stop = 2*np.pi) def spline(x): knot = 3.0 if x &gt;= knot: return x-knot else: return 0.0 output = list(map(spline, my_inputs)) We can visualize the mathematical function by plotting its outputs against its inputs. More information on visualization is given in TODO. map can also be used like mapply. In other words, you can apply it to two containers, import numpy as np x = np.linspace(start = -1., stop = 1.0) y = np.linspace(start = -1., stop = 1.0) def f(x): np.log(x**2 + y**2) output = list(map(spline, my_inputs)) 5.4.2 filter filter is very similar to R’s Filter. It returns an iterable of type filter. raw_data = np.arange(0,1.5,.01) for elem in filter(lambda x : x**2 &gt; 2, raw_data): print(elem) ## 1.42 ## 1.43 ## 1.44 ## 1.45 ## 1.46 ## 1.47 ## 1.48 ## 1.49 This function is also similar to which in R. The only difference is that which does not accept a function, just a logical vector. 5.5 Selected Numpy Functionals Numpy provides a number of functions that facilitate working with np.ndarrays in a functional style. For example, np.apply_along_axis is similar to R’s apply. apply had a MARGIN argument (1 sums rows, 2 sums columns), whereas this function has a axis= argument (0 sums columns, 1 sums rows). import numpy as np my_array = np.arange(6).reshape((2,3)) my_array ## array([[0, 1, 2], ## [3, 4, 5]]) np.apply_along_axis(sum, 0, my_array) # summing columns ## array([3, 5, 7]) np.apply_along_axis(sum, 1, my_array) # summing rows ## array([ 3, 12]) my_array = np.random.normal(size=(10,1000)) np.apply_along_axis(sum, 0, my_array).shape ## (1000,) np.apply_along_axis(sum, 1, my_array).shape ## (10,) 5.6 Functional Methods in Pandas Pandas DataFrames have an .apply method that is very similar to apply in R,1 but again, just like the above function, you have to think about an axis= argument instead of a MARGIN= argument. import pandas as pd alb_real_est = pd.read_csv(&quot;data/albemarle_real_estate.csv&quot;) alb_real_est.shape ## (27943, 12) alb_real_est.apply(len, axis=0) # length of columns ## YearBuilt 27943 ## YearRemodeled 27943 ## Condition 27943 ## NumStories 27943 ## FinSqFt 27943 ## Bedroom 27943 ## FullBath 27943 ## HalfBath 27943 ## TotalRooms 27943 ## LotSize 27943 ## TotalValue 27943 ## City 27943 ## dtype: int64 type(alb_real_est.apply(len, axis=1)) # length of rows ## &lt;class &#39;pandas.core.series.Series&#39;&gt; Another thing to keep in mind is that DataFrames, unlike ndarrays, don’t have to have the same type for all elements. If you have mixed column types, then summing rows, for instance, might not make sense. This just requires subsetting columns before .applying a function to rows. Here is an example of computing each property’s “score.” import pandas as pd # alb_real_est.apply(sum, axis=1) # can&#39;t add letters to numbers! def get_prop_score(row): return 2*row[0] + 3*row[1] alb_real_est[&#39;Score&#39;] = alb_real_est[[&#39;FinSqFt&#39;,&#39;LotSize&#39;]].apply(get_prop_score, 1) alb_real_est[[&#39;FinSqFt&#39;,&#39;LotSize&#39;,&#39;Score&#39;]].head(2) ## FinSqFt LotSize Score ## 0 1922 5.000 3859.000 ## 1 1848 61.189 3879.567 .apply also works with more than one function at a time. alb_real_est[[&#39;FinSqFt&#39;,&#39;LotSize&#39;]].apply([sum, len]) ## FinSqFt LotSize ## sum 55559801 97856.8384 ## len 27943 27943.0000 If you do not want to waste two lines defining a function with def, you can use an anonymous (unnamed) lambda function. Be careful, though–if your function is complex enough, then your lines will get quite wide. For instance, this example is pushing it. alb_real_est[[&#39;FinSqFt&#39;,&#39;LotSize&#39;]].apply(lambda row : sum(row*[2,3]), 1)[:4] ## 0 3859.000 ## 1 3879.567 ## 2 2501.280 ## 3 2639.944 ## dtype: float64 If you want to apply a (scalar-valued) function that takes only individual elements, you should try to use a unary function (recall that this was discussed in TODO). If no such unary function exists, you can apply it with .applymap. alb_real_est[[&#39;FinSqFt&#39;,&#39;LotSize&#39;]].applymap(lambda e : e + 1).head(2) ## FinSqFt LotSize ## 0 1923 6.000 ## 1 1849 62.189 Last, we have a .groupby method, which can be used to mirror the behavior of R’s tapply, aggregate or by. It can take the DataFrame it belongs to, and group its rows into multiple sub-DataFrames. The collection of sub-DataFrames has a lot of the same methods that an individual DataFrame has (e.g. the subsetting operators, and the .apply method), which can all be used in a second step of calculating things on each sub-DataFrame. type(alb_real_est.groupby([&#39;City&#39;])) ## pandas.core.groupby.generic.DataFrameGroupBy type(alb_real_est.groupby([&#39;City&#39;])[&#39;TotalValue&#39;]) ## pandas.core.groupby.generic.SeriesGroupBy Here is an example that models some pretty typical functionality. It shows two ways to get the average home price by city. The first line groups the rows by which City they are in, extracts the TotalValue column in each sub-DataFrame, and then .applys the np.average function on the sole column found in each sub-DataFrame. The second applys a lambda function to each sub-DataFrame directly. alb_real_est.groupby([&#39;City&#39;])[&#39;TotalValue&#39;].apply(np.average) ## City ## CHARLOTTESVILLE 381932.962760 ## CROZET 380425.678927 ## EARLYSVILLE 439140.987124 ## KESWICK 540532.605905 ## NORTH GARDEN 366597.750865 ## SCOTTSVILLE 268407.384615 ## Name: TotalValue, dtype: float64 alb_real_est.groupby([&#39;City&#39;]).apply(lambda df : np.average(df[&#39;TotalValue&#39;])) ## City ## CHARLOTTESVILLE 381932.962760 ## CROZET 380425.678927 ## EARLYSVILLE 439140.987124 ## KESWICK 540532.605905 ## NORTH GARDEN 366597.750865 ## SCOTTSVILLE 268407.384615 ## dtype: float64 More tips on this programming pattern can be found here. 5.7 Functions as Function Outputs in R Functions that create and return other functions are sometimes called function factories. Functions are first class objects in R, so it’s easy to return them. What’s more interesting is that supposedly temporary objects inside the outer function can be accessed during the call of the inner function after it’s returned. Here is a first quick example. funcFactory &lt;- function(greetingMessage){ function(name){ paste(greetingMessage, name) } } greetWithHello &lt;- funcFactory(&quot;Hello&quot;) greetWithHello(&quot;Taylor&quot;) ## [1] &quot;Hello Taylor&quot; greetWithHello(&quot;Charlie&quot;) ## [1] &quot;Hello Charlie&quot; The greetingMessage argument that is passed in, \"Hello\", isn’t temporary anymore. Here is an example that implements a variance reduction technique called common random numbers. Suppose \\(X \\sim \\text{Normal}(\\mu, \\sigma^2)\\), and we are interested in approximating an expectation of a function of this random variable. Suppose that we don’t know that \\[ \\mathbb{E}[\\sin(X)] = \\sin(\\mu) \\exp\\left(-\\frac{\\sigma^2}{2}\\right) \\] for any particular choice of \\(\\mu\\) and \\(\\sigma^2\\), and instead, we choose to use the Monte Carlo method: \\[ \\hat{\\mathbb{E}}[\\sin(X)] = \\frac{1}{n}\\sum_{i=1}^n\\sin(X^i) \\] where \\(X^1, \\ldots, X^n \\overset{\\text{iid}}{\\sim} \\text{Normal}(\\mu, \\sigma^2)\\) is a large collection of draws from the appropriate normal distribution. In real life, the theoretical expectation might not be tractable (either because the random variable has a complicated distribution, or maybe because the functional is very complicated) and Monte Carlo, or some other approximation algorithm, might be our only hope! Here are two functions that calculate the above quantities for \\(n=100\\). actualExpectSin is a function that computes the theoretical expectation for any particular parameter pair. monteCarloSin is a function that implements the Monte Carlo approximate expectation. n &lt;- 1000 # don&#39;t hardcode variables that aren&#39;t passed as arguments! actualExpectSin &lt;- function(params){ stopifnot(params[2] &gt; 0) # second parameter is sigma sin(params[1])*exp(-.5*(params[2]^2)) } monteCarloSin &lt;- function(params){ stopifnot(params[2] &gt; 0) mean(sin(rnorm(n = n, mean = params[1], sd = params[2]))) } # monteCarloSin(c(10,1)) One-off approximations aren’t as interesting as visualizing many expectations for many parameter inputs. On the left, we have the true expectation function plotted with a contour plot. On the right, muGrid &lt;- seq(-10,10, length.out = 100) sigmaGrid &lt;- seq(.001, 5, length.out = 100) muSigmaGrid &lt;- expand.grid(muGrid, sigmaGrid) actuals &lt;- matrix(apply(muSigmaGrid, 1, actualExpectSin), ncol = length(muGrid)) mcApprox &lt;- matrix(apply(muSigmaGrid, 1, monteCarloSin), ncol = length(muGrid)) par(mfrow=c(1,2)) contour(muGrid, sigmaGrid, actuals, xlab = &quot;mu&quot;, ylab = &quot;sigma&quot;, main = &quot;actual expect.s&quot;) contour(muGrid, sigmaGrid, mcApprox, xlab = &quot;mu&quot;, ylab = &quot;sigma&quot;, main = &quot;mc without crn&quot;) par(mfrow=c(1,1)) If we wanted to use common random numbers, we could generate \\(Z^1, \\ldots, Z^n \\overset{\\text{iid}}{\\sim} \\text{Normal}(0, 1)\\), and use the fact that \\[ X^i = \\mu + \\sigma Z^i \\] This leads to the Monte Carlo estimate \\[ \\tilde{\\mathbb{E}}[\\sin(X)] = \\frac{1}{n}\\sum_{i=1}^n\\sin(\\mu + \\sigma Z^i) \\] Here is one function that naively implements Monte Carlo with common random numbers. We generate the collection of standard normal random variates once, globally. Each time you call monteCarloSinCRNv1(c(10,1)), you get the same answer. commonZs &lt;- rnorm(n=n) monteCarloSinCRNv1 &lt;- function(params){ stopifnot(params[2] &gt; 0) mean(sin(params[1] + params[2]*commonZs)) } # monteCarloSinCRNv1(c(10,1)) Let’s compare using common random numbers to going without. As you can see, common random numbers make the plot look “smoother.” mcApproxCRNv1 &lt;- matrix(apply(muSigmaGrid, 1, monteCarloSinCRNv1), ncol = length(muGrid)) par(mfrow=c(1,2)) contour(muGrid, sigmaGrid, mcApprox, xlab = &quot;mu&quot;, ylab = &quot;sigma&quot;, main = &quot;mc without crn&quot;) contour(muGrid, sigmaGrid, mcApproxCRNv1, xlab = &quot;mu&quot;, ylab = &quot;sigma&quot;, main = &quot;mc with crn&quot;) par(mfrow=c(1,1)) The downside to this implementation is that we have a bunch of samples floating around in the global environment. We can implement this much more nicely with a function factory. makeMCFunc &lt;- function(){ commonZs &lt;- rnorm(n=n) function(params){ stopifnot(params[2] &gt; 0) mean(sin(params[1] + params[2]*commonZs)) } } monteCarloSinCRNv2 &lt;- makeMCFunc() # monteCarloSinCRNv2(c(10,1)) Much better! Let’s just make sure this function works by comparing its output to the known true function. 5.8 Functions as Function Outputs in Python Returning functions from functions, explaining mechanics np.vectorize is something that I use quite often. decorators! TODO Both R and Python have reduce functions. R has Reduce, while Python has reduce. In R reduce, 5.8.1 Exercises Let \\(X_t\\) be the price of a stock at time \\(t\\). A call (or put) option is a type of derivative contract you can buy or sell. It’s called a derivative because its value derives from the value of a stock. A single call contract (or put) gives the owner the right to buy (or sell) stock (\\(100\\) shares) at a certain price. This certain price written into an options contract (\\(s\\)) is called the strike price. The day the option expires is called the expiration date. If a holder of the option uses his/her right to buy or sell the underlying stock, he/she is said to have exercised the option. The price and value of options floats around randomly through time just like a stock does. Part of the value of an option is its intrinsic value. It is not the same as the overall value of an option, or its “fair price”–rather, the intrinsic value represents how much money its possible to guarantee at the current moment, if you exercised the option, and then liquidated the resulting position in the underlying. For example, call owners hope the stock rises above the strike price so that they can buy the stock for a low price, and immediately sell it for a high price. If \\(X_t &gt; s\\), they could buy at \\(s\\), and sell at \\(X_t\\). In other words, the intrinsic value of a call option is \\[ C^s_t = 100 \\times \\max(X_t - s, 0) \\] as long as \\(t\\) is before the expiration date. If \\(X_t &lt; s\\), then the owner doesn’t have to buy the stock, so the call at that time is intrinsically worthless. Put owners hope the stock price falls below the strike price, because a put’s intrinsic value is \\[ P^s_t = 100 \\times \\max(s - X_t, 0) \\] Sometimes people buy and sell multiple contracts (with the same expiration date) at the same time. If they do, the intrinsic value of the entire position is additive. For example, if you bought two of the same put option as the one above, then you just double the above expression to get the intrinsic value. As another example, if you buy (you can also sell) a call spread, you buy one call, and then sell another call with the same expiration and a higher strike price. Assuming \\(s_1 &lt; s_2\\) the intrinsic value is of the call spread with strikes \\(s_1\\) and \\(s_2\\) is \\[ 100 \\times \\max(X_t - s_1, 0) - 100\\times\\max(X_t - s_2, 0) \\] In R, write a function called getIntrinsicValueFunc that takes three vectors as arguments, (dirs, strikes, contractType, numContracts), and returns a function. These arguments should all be the same length. The length of these inputs is the number of contracts in an options position. The function that getIntrinsicValueFunc returns should only take one argument: prices, a vector of prices of the underlying stock. The function that getIntrinsicValueFunc should return a vector of intrinsic values over time. Because the arguments may only take on certain values, and because the function to be returned has a certain signature, a template has been provided below. getIntrinsicValueFunc &lt;-function(dirs, strikes, contractType, numContracts){ stopifnot(all(dirs %in% c(&quot;buy&quot;, &quot;sell&quot;))) stopifnot(all(contractType %in% c(&quot;call&quot;, &quot;put&quot;))) stopifnot(all(is.integer(strikes))) stopifnot(all(is.integer(numContracts))) stopifnot( (length(dirs) == length(strikes)) &amp;&amp; (length(dirs) == length(contractType))) usefulFunc &lt;- function(prices){ # TODO } return(usefulFunc) } Create an intrinsic value plot. The x-axis should be a grid of values: let’s say from \\(100\\) to \\(200\\). On the y-axis plot the intrinsic value of buying a call spread with strikes \\(110\\) and \\(120\\). Do the same thing as above but show what happens when you sell the same call spread. Plot the intrinsic value of a bought put spread with strikes \\(110\\) and \\(120\\). Plot the intrinsic value of a sold put spread with strikes \\(110\\) and \\(120\\). What is the difference between all four plots? Do the same thing, but in Python! creating row masks in a data frame .apply with weird predicate function Optimization example Importance sampling example bootstrap numerical integration You should know that a lot of special-case functions that you typically apply to rows or columns come built-in as DataFrame methods. For instance, .mean would allow you to do something like my_df.mean().↩︎ "]]
